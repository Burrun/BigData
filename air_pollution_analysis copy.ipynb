{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "%pip install lightgbm shap matplotlib\n",
    "%pip install statsmodels\n",
    "%pip install scikit-learn\n",
    "%pip install display\n",
    "%pip install xmltodict  \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국의 미세먼지 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 공통 파일명 패턴 앞뒤\n",
    "base_path = \"./data\"\n",
    "file_name_format = \"{}년 대기오염물질 배출량 통계(시군구별 배출원소분류별 연료별).xlsx\"\n",
    "\n",
    "# 시도명 축약 매핑\n",
    "name_map = {\n",
    "    \"서울특별시\": \"서울\", \"부산광역시\": \"부산\", \"대구광역시\": \"대구\", \"인천광역시\": \"인천\",\n",
    "    \"광주광역시\": \"광주\", \"대전광역시\": \"대전\", \"울산광역시\": \"울산\", \"세종특별자치시\": \"세종\",\n",
    "    \"경기도\": \"경기\", \"강원도\": \"강원\", \"충청북도\": \"충북\", \"충청남도\": \"충남\",\n",
    "    \"전라북도\": \"전북\", \"전라남도\": \"전남\", \"경상북도\": \"경북\", \"경상남도\": \"경남\",\n",
    "    \"제주특별자치도\": \"제주\" , \"제주도\": \"제주\" , \"바다\":\"바다\"\n",
    "}\n",
    "\n",
    "release_pm25 = {}\n",
    "release_pm10 = {}\n",
    "\n",
    "for year in range(2015, 2023):\n",
    "    file_path = os.path.join(base_path, file_name_format.format(year))\n",
    "    \n",
    "    df_raw = pd.read_excel(file_path)\n",
    "\n",
    "    # 상위 3행 제거 및 헤더 생성\n",
    "    df = df_raw.iloc[3:].reset_index(drop=True)\n",
    "    columns_1 = df_raw.iloc[1].tolist()\n",
    "    columns_2 = df_raw.iloc[2].tolist()\n",
    "    final_columns = [a if pd.notna(a) else b for a, b in zip(columns_1, columns_2)]\n",
    "    df.columns = final_columns\n",
    "\n",
    "    # 필요한 열 추출\n",
    "    df_filtered = df[[\"시도\", \"배출원대분류\", \"PM-2.5\", \"PM-10\"]].copy()\n",
    "    df_filtered[\"PM-2.5\"] = pd.to_numeric(df_filtered[\"PM-2.5\"], errors=\"coerce\")\n",
    "    df_filtered[\"PM-10\"] = pd.to_numeric(df_filtered[\"PM-10\"], errors=\"coerce\")\n",
    "    df_filtered = df_filtered.dropna(subset=[\"시도\", \"배출원대분류\", \"PM-2.5\", \"PM-10\"])\n",
    "\n",
    "\n",
    "    # 그룹화 및 피벗 테이블 생성\n",
    "    summary_pm25 = df_filtered.groupby([\"시도\", \"배출원대분류\"])[\"PM-2.5\"].sum().reset_index()\n",
    "    summary_pm10 = df_filtered.groupby([\"시도\", \"배출원대분류\"])[\"PM-10\"].sum().reset_index()\n",
    "\n",
    "    pivot_pm25 = summary_pm25.pivot(index=\"시도\", columns=\"배출원대분류\", values=\"PM-2.5\").fillna(0).astype(int)\n",
    "    pivot_pm10 = summary_pm10.pivot(index=\"시도\", columns=\"배출원대분류\", values=\"PM-10\").fillna(0).astype(int)\n",
    "\n",
    "    # 총합 및 시도 정리\n",
    "    pivot_pm25[\"총합\"] = pivot_pm25.sum(axis=1)\n",
    "    pivot_pm10[\"총합\"] = pivot_pm10.sum(axis=1)\n",
    "    pivot_pm25 = pivot_pm25.reset_index()\n",
    "    pivot_pm10 = pivot_pm10.reset_index()\n",
    "    pivot_pm25[\"시도\"] = pivot_pm25[\"시도\"].map(name_map)\n",
    "    pivot_pm10[\"시도\"] = pivot_pm10[\"시도\"].map(name_map)\n",
    "    for col in [\"농업\", \"에너지수송 및 저장\", \"유기용제 사용\"]:\n",
    "        if col in pivot_pm25.columns:\n",
    "            pivot_pm25 = pivot_pm25.drop(columns=[col])\n",
    "        if col in pivot_pm10.columns:\n",
    "            pivot_pm10 = pivot_pm10.drop(columns=[col])\n",
    "\n",
    "    pivot_pm25.to_csv(f'processed_data/res_{year}_pm2.5.csv', index=False)\n",
    "    pivot_pm10.to_csv(f'processed_data/res_{year}_pm10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data 폴더의 연도별 배출량 csv 파일 목록\n",
    "proc_dir = 'processed_data'\n",
    "proc_files = [f for f in os.listdir(proc_dir) if f.endswith('.csv') and f.startswith('res_')]\n",
    "\n",
    "# 연도별 배출량 데이터 병합\n",
    "release_list = []\n",
    "for f in proc_files:\n",
    "    year = int(f.split('_')[1])\n",
    "    pm_type = 'pm2.5' if 'pm2.5' in f else 'pm10'\n",
    "    df = pd.read_csv(os.path.join(proc_dir, f))\n",
    "    df['year'] = year\n",
    "    df['pm_type'] = pm_type\n",
    "    release_list.append(df)\n",
    "release_df = pd.concat(release_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# 8년치 PM 농도 데이터 (wide -> long)\n",
    "pm25 = pd.read_csv('data/8y_pm2.5.csv')\n",
    "pm10 = pd.read_csv('data/8y_pm10.csv')\n",
    "pm25 = pm25.rename(columns={'시/도': '시도'})\n",
    "pm10 = pm10.rename(columns={'시/도': '시도'})\n",
    "pm25_long = pm25.melt(id_vars=['시도'], var_name='year', value_name='농도')\n",
    "pm10_long = pm10.melt(id_vars=['시도'], var_name='year', value_name='농도')\n",
    "pm25_long['pm_type'] = 'pm2.5'\n",
    "pm10_long['pm_type'] = 'pm10'\n",
    "pm_long = pd.concat([pm25_long, pm10_long], ignore_index=True)\n",
    "pm_long['year'] = pm_long['year'].astype(int) + 2000\n",
    "\n",
    "# release_df와 pm_long을 year, 시도, pm_type 기준으로 merge\n",
    "df_master = pd.merge(release_df, pm_long, on=['시도', 'year', 'pm_type'], how='inner')\n",
    "# 결과 저장\n",
    "df_master.to_csv('df_master.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.상관관계 및 다중 공산성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df = pd.read_csv('df_master.csv')\n",
    "\n",
    "# 오염원 컬럼만 추출 (시도, year, pm_type, PM2.5/PM10 등 제외)\n",
    "exclude_cols = ['시도', 'year', 'pm_type', 'PM2.5','총합','농도','PM10']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# 상관계수 행렬 계산\n",
    "corr_df = df[feature_cols].corr(method='pearson')\n",
    "corr_df.to_csv('corr_df.csv') \n",
    "corr_df = pd.read_csv('corr_df.csv', index_col=0)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_df, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Pearson Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('corr_heatmap.png')\n",
    "plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('df_master.csv')\n",
    "# 오염원 컬럼만 추출 (시도, year, pm_type, PM2.5, PM10 등 제외)\n",
    "exclude_cols = ['시도', 'year', 'pm_type', 'PM2.5','총합','농도','PM10']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "X = df[feature_cols]\n",
    "# VIF 계산\n",
    "data = []\n",
    "for i, col in enumerate(X.columns):\n",
    "    vif = variance_inflation_factor(X.values, i)\n",
    "    data.append({'feature': col, 'VIF': vif})\n",
    "vif_df = pd.DataFrame(data)\n",
    "vif_df.to_csv('vif_table.csv', index=False) \n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **같은 배출량이더라도, 오염원의 배출 위치, 방식, 화학 성분, 기상 조건에 따라 대기질 영향은 달라진다**는 것은  \n",
    "> 환경과학에서 인정된 객관적 사실입니다.\n",
    "\n",
    "따라서, 단순히 `t/yr` 배출량만으로 대기질 기여도를 판단하기보다는,  \n",
    "**질량 기준 + 배출 특성 + 기상 조건**을 함께 고려해야 합니다.\n",
    "\n",
    "| 오염원   | PM2.5 배출량 | 배출 조건             | 대기질 영향              |\n",
    "|----------|---------------|------------------------|---------------------------|\n",
    "| 발전소   | 100kg         | 고고도, 확산 양호       | 낮음                      |\n",
    "| 차량     | 100kg         | 지표면, 교통 밀집        | 매우 높음                 |\n",
    "| 가정 난방 | 100kg         | 겨울철 밤, 지면 근처     | 높음                      |\n",
    "| 농업 연소 | 100kg         | 야외, 바람 강함          | 지역에 따라 낮을 수도 있음 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge: 변수 간 상관·과적합을 완화하면서 모든 오염원별 영향 계수를 얻기 위한 선택.\n",
    "\n",
    "RidgeCV + cross_val_predict:\n",
    "\n",
    "RidgeCV → 최적 α 선택.\n",
    "\n",
    "cross_val_predict → 훈련에 쓰이지 않은 예측으로 공정한 RMSE·R² 산출.\n",
    "\n",
    "이렇게 하면 “계수”와 “신뢰할 성능 지표”를 동시에 확보해,\n",
    "\n",
    "계수: 정책용 가중치 해석\n",
    "\n",
    "RMSE‧R²: 모델이 실측을 얼마나 잘 설명하는지 품질 체크\n",
    "가 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① PM10 모델에서 보이는 것\n",
    "배출원\t계수\t해석 (정성)\n",
    "도로이동오염원\t+4.85\t가장 강력한 양(+) 영향원 ⇒ 도로교통 배출 1 σ↑ → PM10 약 +4.9 µg/m³\n",
    "비도로이동오염원\t–2.13\t도로·연소류와 공선성 → 부호가 음으로 뒤집힘\n",
    "비산먼지\t–0.98\t간접 효과·계수 수축으로 음\n",
    "비산업 연소\t+1.46\t두 번째로 큰 양(+) 기여\n",
    "기타 연소·공정\t±1 이하\t영향 약함 or 다른 변수에 흡수\n",
    "\n",
    "α = 6.43 → 가중치가 꽤 ‘완만하게’(over-fitting 방지용으로) 축소되었다는 뜻.\n",
    "\n",
    "② PM2.5 모델에서 보이는 것\n",
    "배출원\t계수\t특징\n",
    "도로이동오염원\t+2.95\t여전히 최상위 영향\n",
    "제조업 연소\t+0.70\tPM10보다 상대적으로 ↑\n",
    "비산먼지·기타\t음(–)\t위와 동일한 해석\n",
    "\n",
    "α = 5.46 → PM10보다 약간 작은 규제 → 계수가 조금 더 자유롭게 변함.\n",
    "\n",
    "지표\tPM10\tPM2.5\t해석\n",
    "RMSE (µg/㎥)\t5.72\t3.92\t\n",
    "\n",
    "평균 농도 대비 약 15 %(=5.7/39)·18 %(=3.9/21.8) 오차 → 경향 파악·우선순위엔 쓸 만하지만, 정밀 예측용으론 부족\n",
    "\n",
    "R²\t0.25\t0.15\t분산의 25 %·15 % 설명 → “중간 이하” 수준의 설명력\n",
    "α = 1000\t두 모델 모두 매우 강한 규제 → 계수 축소가 크고, 예측 성능도 **제한적**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 날씨 요인 데이터 로드 및 전처리(시계열 데이터 , 풍속, 상대습도, 평균기온)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, xmltodict, json, time\n",
    "\n",
    "API_KEY = \"iZg1DoAwxAk6RWJBOuGLXve+S0ah2zN9LwseR4wHfOJVxQgqfyTVPeIBs39M6h7bJ608kO8ZsiC2iEbMaOJX0Q==\"\n",
    "DATE_FMT = \"%Y%m%d\"\n",
    "\n",
    "# ────────────────────────── 시·도 → 지점 번호 ──────────────────────────\n",
    "station_map = {\n",
    "    \"서울\": 108,  \"부산\": 159, \"대구\": 143, \"인천\": 112,\n",
    "    \"광주\": 156,  \"대전\": 133, \"울산\": 152,\n",
    "    \"경기\": 119,  \"강원\": 101, \"충북\": 131, \"충남\": 236,\n",
    "    \"전북\": 146,  \"전남\": 165, \"경북\": 137, \"경남\": 155,\n",
    "    \"제주\": 184,\n",
    "}\n",
    "\n",
    "# ────────────────────────── API 호출 함수 ──────────────────────────\n",
    "def fetch_asos_daily(city: str, stn: int,\n",
    "                    start_year: int = 2015, end_year: int = 2022) -> pd.DataFrame:\n",
    "    base = \"http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList\"\n",
    "    rows = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        params = {\n",
    "            \"ServiceKey\": API_KEY,        # Decoding 키 그대로\n",
    "            \"dataType\":   \"JSON\",         # JSON 우선\n",
    "            \"dataCd\":     \"ASOS\",\n",
    "            \"dateCd\":     \"DAY\",\n",
    "            \"startDt\":    f\"{year}0101\",\n",
    "            \"endDt\":      f\"{year}1231\",\n",
    "            \"stnIds\":     stn,\n",
    "            \"pageNo\":     1,\n",
    "            \"numOfRows\":  365,\n",
    "        }\n",
    "\n",
    "        r = requests.get(base, params=params, timeout=15)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        # JSON ↔ XML 자동 판별\n",
    "        if \"application/json\" in r.headers.get(\"Content-Type\", \"\"):\n",
    "            data = r.json()\n",
    "        else:\n",
    "            data = json.loads(json.dumps(xmltodict.parse(r.text)))\n",
    "\n",
    "        items = data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "        rows.extend(items)\n",
    "        time.sleep(0.15)   # 쿼터 여유\n",
    "\n",
    "    df = pd.json_normalize(rows).rename(columns={\n",
    "        \"tm\":     \"date\",\n",
    "        \"n99Rn\":  \"rain_mm\",\n",
    "        \"avgRhm\": \"rh\",\n",
    "        \"avgTa\":  \"ta\",\n",
    "        \"avgWs\":  \"ws\",\n",
    "    })\n",
    "\n",
    "    df[\"city\"] = city\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    # rain_mm: 결측값을 0.0으로 대체\n",
    "    df[\"rain_mm\"] = pd.to_numeric(df[\"rain_mm\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # 나머지 수치 컬럼은 그대로 numeric 변환 (결측치 유지)\n",
    "    df[[\"rh\", \"ta\", \"ws\"]] = df[[\"rh\", \"ta\", \"ws\"]].apply(\n",
    "        pd.to_numeric, errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return df[[\"city\", \"date\", \"rain_mm\", \"rh\", \"ta\", \"ws\"]]\n",
    "\n",
    "# ────────────────────────── 전체 시·도 루프 ──────────────────────────\n",
    "frames = []\n",
    "for city, stn in station_map.items():\n",
    "    print(f\"↳ {city}({stn}) 다운로드 중…\")\n",
    "    frames.append(fetch_asos_daily(city, stn))\n",
    "\n",
    "kr_meteo_2015_2022 = (\n",
    "    pd.concat(frames)\n",
    "    .sort_values([\"city\", \"date\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(kr_meteo_2015_2022.head())\n",
    "print(\"총 행:\", len(kr_meteo_2015_2022))\n",
    "\n",
    "kr_meteo_2015_2022.to_csv(\"KOR_met_2015_2022.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 중국/한국의 미세먼지 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 폴더 내 모든 air-quality.csv 파일 탐색\n",
    "data_dir = 'data'\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith('air-quality.csv')]\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in files:\n",
    "    city = file.split('-')[0]  # 파일명에서 도시명 추출\n",
    "    df = pd.read_csv(os.path.join(data_dir, file))\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    for _, row in df.iterrows():\n",
    "        records.append({\n",
    "            'city': city,\n",
    "            'date': row['date'],\n",
    "            'value': {'pm25': row['pm25'], 'pm10': row['pm10']}\n",
    "        })\n",
    "\n",
    "# 2. records로부터 DataFrame 생성\n",
    "weather_df = pd.DataFrame(records)\n",
    "\n",
    "# 3. date가 2015~2022년까지만 필터링\n",
    "weather_df = weather_df[\n",
    "    (weather_df['date'].dt.year >= 2015) & (weather_df['date'].dt.year <= 2022)\n",
    "]\n",
    "\n",
    "# 4. 중국 주요 도시 리스트\n",
    "china_cities = ['상하이', '청도', '텐진', '베이징']\n",
    "\n",
    "# 5. 한국 도시만 필터링 (중국 도시 제외)\n",
    "weather_df_kr = weather_df[~weather_df['city'].isin(china_cities)]\n",
    "\n",
    "# 6. pivot: index=date, columns=city, values=value(object) - 한국\n",
    "pivot_df_kr = weather_df_kr.pivot(index='date', columns='city', values='value')\n",
    "pivot_df_kr.to_csv(\"PM_KR_2015_2022.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 7. 중국 도시만 따로 저장\n",
    "weather_df_cn = weather_df[weather_df['city'].isin(china_cities)]\n",
    "pivot_df_cn = weather_df_cn.pivot(index='date', columns='city', values='value')\n",
    "pivot_df_cn.to_csv(\"PM_CN_2015_2022.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, warnings, json, os, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 한글 폰트 설정\n",
    "from matplotlib import font_manager as fm \n",
    "for font in [\"Malgun Gothic\", \"NanumGothic\", \"AppleGothic\"]:\n",
    "    if any(font in f.name for f in fm.fontManager.ttflist):\n",
    "        plt.rcParams[\"font.family\"] = font\n",
    "        break\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 시각화 스타일 설정\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# 경로 설정\n",
    "DATA = Path(\"./\")\n",
    "OUT_PM25 = Path(\"./domestic_pm25_improved\")\n",
    "OUT_PM10 = Path(\"./domestic_pm10_improved\")\n",
    "OUT_PM25.mkdir(exist_ok=True)\n",
    "OUT_PM10.mkdir(exist_ok=True)\n",
    "\n",
    "# 로깅 설정\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('regression_model.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───── 1. 데이터 로드 및 전처리 함수 ─────\n",
    "def load_data():\n",
    "    \"\"\"데이터 파일 로드 및 기본 전처리\"\"\"\n",
    "    logger.info(\"데이터 로드 시작\")\n",
    "    \n",
    "    try:\n",
    "        pm_kr_raw = pd.read_csv(DATA/\"PM_KR_2015_2022.csv\")\n",
    "        pm_cn_raw = pd.read_csv(DATA/\"PM_CN_2015_2022.csv\")\n",
    "        met_kr = pd.read_csv(DATA/\"KOR_met_2015_2022.csv\", parse_dates=[\"date\"])\n",
    "        master = pd.read_csv(DATA/\"df_master.csv\")\n",
    "        \n",
    "        # 데이터 기본 정보 로깅\n",
    "        logger.info(f\"PM_KR 데이터: {pm_kr_raw.shape}, PM_CN 데이터: {pm_cn_raw.shape}\")\n",
    "        logger.info(f\"기상 데이터: {met_kr.shape}, 마스터 데이터: {master.shape}\")\n",
    "        \n",
    "        return pm_kr_raw, pm_cn_raw, met_kr, master\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"데이터 로드 중 오류 발생: {e}\")\n",
    "        raise\n",
    "\n",
    "def tidy_pm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"딕셔너리 문자열로 된 PM 값을 long-format & 숫자형으로 변환\"\"\"\n",
    "    logger.info(f\"PM 데이터 정리 시작: {df.shape}\")\n",
    "    \n",
    "    melted = df.melt(id_vars=\"date\", var_name=\"city\", value_name=\"raw\")\n",
    "    melted[\"date\"] = pd.to_datetime(melted[\"date\"])\n",
    "\n",
    "    def extract(s: str, key: str):\n",
    "        if not isinstance(s, str):\n",
    "            return np.nan\n",
    "        try:\n",
    "            val = ast.literal_eval(s.replace(\"'\", '\"')).get(key, \"\").strip()\n",
    "            return pd.to_numeric(val, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    melted[\"pm25\"] = melted[\"raw\"].map(lambda x: extract(x, \"pm25\"))\n",
    "    melted[\"pm10\"] = melted[\"raw\"].map(lambda x: extract(x, \"pm10\"))\n",
    "    \n",
    "    # 결측치 비율 확인 및 로깅\n",
    "    pm25_missing = melted[\"pm25\"].isna().mean() * 100\n",
    "    pm10_missing = melted[\"pm10\"].isna().mean() * 100\n",
    "    logger.info(f\"PM2.5 결측치 비율: {pm25_missing:.2f}%, PM10 결측치 비율: {pm10_missing:.2f}%\")\n",
    "    \n",
    "    return melted.drop(columns=\"raw\")\n",
    "\n",
    "def interpolate_city(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"시계열 보간 (city 단위)\"\"\"\n",
    "    logger.info(\"시계열 보간 시작\")\n",
    "    \n",
    "    out = []\n",
    "    for c, g in df.groupby(\"city\"):\n",
    "        g = g.sort_values(\"date\").set_index(\"date\")\n",
    "        \n",
    "        # 보간 전 결측치 수 확인\n",
    "        missing_before = g[[\"pm25\", \"pm10\"]].isna().sum()\n",
    "        \n",
    "        # 시계열 보간 적용\n",
    "        g[[\"pm25\", \"pm10\"]] = g[[\"pm25\", \"pm10\"]].interpolate(method=\"time\", limit=3)\n",
    "        \n",
    "        # 보간 후 결측치 수 확인\n",
    "        missing_after = g[[\"pm25\", \"pm10\"]].isna().sum()\n",
    "        \n",
    "        # 결과 로깅\n",
    "        logger.debug(f\"도시 {c} - 보간 전 결측치: {missing_before.to_dict()}, 보간 후: {missing_after.to_dict()}\")\n",
    "        \n",
    "        g[\"city\"] = c\n",
    "        out.append(g.reset_index())\n",
    "    \n",
    "    result = pd.concat(out, ignore_index=True)\n",
    "    logger.info(f\"시계열 보간 완료: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "def prepare_china_features(pm_cn: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"중국 도시별 PM 데이터에서 시차(lag) 특성 생성\"\"\"\n",
    "    logger.info(\"중국 시차 특성 생성 시작\")\n",
    "    \n",
    "    # 피벗 테이블 생성\n",
    "    pm_cn_pivot = pm_cn.pivot_table(index=\"date\", columns=\"city\", values=[\"pm25\", \"pm10\"])\n",
    "    pm_cn_pivot.columns = [f\"{city}_{var}\" for var, city in pm_cn_pivot.columns]\n",
    "    \n",
    "    # 기본 데이터프레임 생성\n",
    "    pm_cn_features = pm_cn_pivot.copy()\n",
    "    \n",
    "    # 시차(lag) 특성 추가 (1일, 2일, 3일)\n",
    "    for lag in [1, 2, 3]:\n",
    "        lagged = pm_cn_pivot.shift(lag)\n",
    "        lagged.columns = [f\"{col}_lag{lag}\" for col in pm_cn_pivot.columns]\n",
    "        pm_cn_features = pd.concat([pm_cn_features, lagged], axis=1)\n",
    "    \n",
    "    # 이동 평균 특성 추가 (3일, 7일)\n",
    "    for window in [3, 7]:\n",
    "        rolled = pm_cn_pivot.rolling(window=window).mean()\n",
    "        rolled.columns = [f\"{col}_ma{window}\" for col in pm_cn_pivot.columns]\n",
    "        pm_cn_features = pd.concat([pm_cn_features, rolled], axis=1)\n",
    "    \n",
    "    logger.info(f\"중국 시차 특성 생성 완료: {pm_cn_features.shape}\")\n",
    "    return pm_cn_features.reset_index()\n",
    "\n",
    "def process_master_data(master: pd.DataFrame) -> tuple:\n",
    "    \"\"\"마스터 데이터 처리 및 일별 데이터로 보간\"\"\"\n",
    "    logger.info(\"마스터 데이터 처리 시작\")\n",
    "    \n",
    "    # PM10과 PM2.5 데이터 분리\n",
    "    master_pm10 = master[master['pm_type'] == 'pm10'].copy()\n",
    "    master_pm25 = master[master['pm_type'] == 'pm2.5'].copy()\n",
    "    \n",
    "    # 날짜 변환\n",
    "    master_pm10[\"date\"] = pd.to_datetime(master_pm10[\"year\"].astype(str) + \"-01-01\")\n",
    "    master_pm25[\"date\"] = pd.to_datetime(master_pm25[\"year\"].astype(str) + \"-01-01\")\n",
    "    \n",
    "    # 불필요한 열 제거\n",
    "    drop_cols = [\"year\", \"총합\", \"농도\", \"pm_type\"]\n",
    "    master_pm10 = master_pm10.drop(columns=drop_cols)\n",
    "    master_pm25 = master_pm25.drop(columns=drop_cols)\n",
    "    \n",
    "    # 일별 데이터로 보간\n",
    "    master_daily_pm10 = interpolate_to_daily(master_pm10)\n",
    "    master_daily_pm25 = interpolate_to_daily(master_pm25)\n",
    "    \n",
    "    logger.info(f\"마스터 데이터 처리 완료: PM10={master_daily_pm10.shape}, PM2.5={master_daily_pm25.shape}\")\n",
    "    return master_daily_pm10, master_daily_pm25\n",
    "\n",
    "def interpolate_to_daily(df):\n",
    "    \"\"\"연간 데이터를 일별 데이터로 보간\"\"\"\n",
    "    agg = (df.groupby([\"시도\", \"date\"]).mean(numeric_only=True)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"시도\": \"city\"}))\n",
    "    \n",
    "    daily_blocks = []\n",
    "    for c, g in agg.groupby(\"city\"):\n",
    "        g = g.set_index(\"date\")\n",
    "        \n",
    "        # 다음 연도 1월 1일까지 일자 생성\n",
    "        full_idx = pd.date_range(g.index.min(),\n",
    "                    g.index.max() + pd.offsets.YearEnd(0),\n",
    "                    freq=\"D\")\n",
    "        g = g.reindex(full_idx)\n",
    "        \n",
    "        # 선형 보간\n",
    "        g = g.interpolate(method=\"linear\")\n",
    "        \n",
    "        g[\"city\"] = c\n",
    "        daily_blocks.append(g.reset_index().rename(columns={\"index\": \"date\"}))\n",
    "    \n",
    "    return pd.concat(daily_blocks, ignore_index=True)\n",
    "\n",
    "def merge_datasets(pm_kr, met_kr, master_daily, pm_cn_features, target_type):\n",
    "    \"\"\"데이터셋 병합\"\"\"\n",
    "    logger.info(f\"{target_type} 데이터셋 병합 시작\")\n",
    "    \n",
    "    merged_df = (pm_kr\n",
    "              .merge(met_kr, on=[\"city\", \"date\"], how=\"left\")\n",
    "              .merge(master_daily, on=[\"city\", \"date\"], how=\"left\")\n",
    "              .merge(pm_cn_features, on=\"date\", how=\"left\")\n",
    "             )\n",
    "    \n",
    "    # 결측치 비율 확인\n",
    "    missing_ratio = merged_df.isna().mean().sort_values(ascending=False)\n",
    "    high_missing = missing_ratio[missing_ratio > 0.3]\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        logger.warning(f\"높은 결측치 비율(>30%)을 가진 특성: {high_missing.index.tolist()}\")\n",
    "    \n",
    "    logger.info(f\"{target_type} 데이터셋 병합 완료: {merged_df.shape}\")\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───── 2. 특성 엔지니어링 함수 ─────\n",
    "def engineer_features(df):\n",
    "    \"\"\"추가 특성 엔지니어링\"\"\"\n",
    "    logger.info(\"특성 엔지니어링 시작\")\n",
    "    \n",
    "    # 날짜 관련 특성\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "    df['season'] = df['month'].map(lambda m: 1 if 3 <= m <= 5 else 2 if 6 <= m <= 8 else 3 if 9 <= m <= 11 else 4)\n",
    "    \n",
    "    # 기상 관련 파생 특성\n",
    "    if 'rh' in df.columns and 'ta' in df.columns:\n",
    "        # 불쾌지수 계산 (Discomfort Index)\n",
    "        df['discomfort_idx'] = 0.81 * df['ta'] + 0.01 * df['rh'] * (0.99 * df['ta'] - 14.3) + 46.3\n",
    "        \n",
    "        # 체감온도 계산 (간단한 버전)\n",
    "        df['feels_like'] = df['ta'] - 0.2 * (100 - df['rh']) / 5\n",
    "    \n",
    "    # 풍속 구간화\n",
    "    if 'ws' in df.columns:\n",
    "        df['wind_category'] = pd.cut(df['ws'], bins=[0, 0.5, 2, 3.5, 5, 100], \n",
    "                                    labels=['calm', 'light', 'moderate', 'strong', 'very_strong'])\n",
    "        \n",
    "        # 원-핫 인코딩\n",
    "        wind_dummies = pd.get_dummies(df['wind_category'], prefix='wind')\n",
    "        df = pd.concat([df, wind_dummies], axis=1)\n",
    "    \n",
    "    # 도시 원-핫 인코딩\n",
    "    city_dummies = pd.get_dummies(df['city'], prefix='city')\n",
    "    df = pd.concat([df, city_dummies], axis=1)\n",
    "    \n",
    "    # 상호작용 특성\n",
    "    if 'rh' in df.columns and 'ws' in df.columns:\n",
    "        df['rh_ws_interaction'] = df['rh'] * df['ws']\n",
    "    \n",
    "    logger.info(f\"특성 엔지니어링 완료: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def select_features(df, target):\n",
    "    \"\"\"특성 선택 및 타겟 준비\"\"\"\n",
    "    logger.info(f\"{target} 특성 선택 시작\")\n",
    "    \n",
    "    # 제외할 열\n",
    "    exclude = [\"pm25\", \"pm10\", \"date\", \"wind_category\"]\n",
    "    \n",
    "    # 타겟에 따라 추가 제외 열 설정\n",
    "    if target == \"pm10\":\n",
    "        exclude += [c for c in df.columns if \"_pm25\" in c]\n",
    "    elif target == \"pm25\":\n",
    "        exclude += [c for c in df.columns if \"_pm10\" in c]\n",
    "    \n",
    "    # 범주형 특성을 범주형으로 변환\n",
    "    df[\"city\"] = df[\"city\"].astype(\"category\")\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = df[\"season\"].astype(\"category\")\n",
    "    \n",
    "    # 특성 선택\n",
    "    features = [c for c in df.columns if c not in exclude]\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    logger.info(f\"{target} 특성 선택 완료: X={X.shape}, y={y.shape}\")\n",
    "    logger.info(f\"{target} 결측치 없는 타겟 행: {y.notna().sum():,}\")\n",
    "    \n",
    "    return X, y, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(X, y, cv):\n",
    "    \"\"\"하이퍼파라미터 최적화\"\"\"\n",
    "    logger.info(\"하이퍼파라미터 최적화 시작\")\n",
    "    \n",
    "    # 결측치 처리를 위한 간단한 전처리 파이프라인\n",
    "    X_sample = X.sample(min(10000, len(X)))  # 최적화를 위한 샘플링\n",
    "    y_sample = y.loc[X_sample.index]\n",
    "    \n",
    "    # 유효한 행만 선택\n",
    "    valid_idx = y_sample.notna()\n",
    "    X_sample = X_sample[valid_idx]\n",
    "    y_sample = y_sample[valid_idx]\n",
    "    \n",
    "    # 하이퍼파라미터 탐색 공간\n",
    "    param_distributions = {\n",
    "        'num_leaves': [31, 63, 127],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.1, 0.5],\n",
    "        'min_child_samples': [5, 10, 20]\n",
    "    }\n",
    "    \n",
    "    # 랜덤 탐색 교차 검증\n",
    "    model = lgb.LGBMRegressor(objective='regression', random_state=42)\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=10,  # 10회 시도\n",
    "        cv=cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    search.fit(X_sample, y_sample)\n",
    "    \n",
    "    logger.info(f\"최적 하이퍼파라미터: {search.best_params_}\")\n",
    "    logger.info(f\"최적 RMSE: {-search.best_score_:.4f}\")\n",
    "    \n",
    "    return search.best_params_\n",
    "\n",
    "def train_evaluate_model(X, y, best_params, output_dir, target, cv_splits=5):\n",
    "    \"\"\"모델 학습 및 평가\"\"\"\n",
    "    logger.info(f\"{target} 모델 학습 및 평가 시작\")\n",
    "    \n",
    "    # 시계열 교차 검증 설정\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "    \n",
    "    # 교차 검증으로 best_iteration 추정\n",
    "    best_iters = []\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (tr, va) in enumerate(tscv.split(X)):\n",
    "        logger.info(f\"교차 검증 {fold+1}/{cv_splits} 시작\")\n",
    "        \n",
    "        # 유효한 행만 선택 (결측치 제외)\n",
    "        valid_tr = y.iloc[tr].notna()\n",
    "        valid_va = y.iloc[va].notna()\n",
    "        \n",
    "        X_tr = X.iloc[tr[valid_tr]]\n",
    "        y_tr = y.iloc[tr[valid_tr]]\n",
    "        X_va = X.iloc[va[valid_va]]\n",
    "        y_va = y.iloc[va[valid_va]]\n",
    "        \n",
    "        # 모델 설정\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            **best_params,\n",
    "            n_estimators=2000,  # 넉넉히 설정 (early stopping 사용)\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 학습\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "        )\n",
    "        \n",
    "        # 최적 반복 횟수 저장\n",
    "        best_iters.append(model.best_iteration_)\n",
    "        \n",
    "        # 검증 세트 평가\n",
    "        y_pred = model.predict(X_va)\n",
    "        rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "        cv_scores.append(rmse)\n",
    "        \n",
    "        logger.info(f\"교차 검증 {fold+1} RMSE: {rmse:.4f}, 최적 반복 횟수: {model.best_iteration_}\")\n",
    "    \n",
    "    # 최종 best_iteration 결정\n",
    "    best_iter = int(np.median(best_iters))\n",
    "    mean_cv_rmse = np.mean(cv_scores)\n",
    "    \n",
    "    logger.info(f\"교차 검증 평균 RMSE: {mean_cv_rmse:.4f}\")\n",
    "    logger.info(f\"최종 선택된 반복 횟수: {best_iter}\")\n",
    "    \n",
    "    # 전체 데이터로 최종 모델 학습\n",
    "    final_model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        **best_params,\n",
    "        n_estimators=best_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 유효한 행만 선택\n",
    "    valid_idx = y.notna()\n",
    "    X_valid = X[valid_idx]\n",
    "    y_valid = y[valid_idx]\n",
    "    \n",
    "    final_model.fit(X_valid, y_valid)\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_path = output_dir / f\"lgbm_model_{target}.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    \n",
    "    # 텍스트 형식으로도 저장\n",
    "    model_txt = final_model.booster_.model_to_string(num_iteration=-1)\n",
    "    with open(output_dir / f\"lgbm_model_{target}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model_txt)\n",
    "    \n",
    "    logger.info(f\"모델 저장 완료: {model_path}\")\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    y_pred = final_model.predict(X)\n",
    "    \n",
    "    # 타깃·예측 모두 유효한(=NaN 아님) 행만 남김\n",
    "    valid = (~np.isnan(y_pred)) & (y.notna())\n",
    "    y_true = y[valid]\n",
    "    y_pred_valid = y_pred[valid]\n",
    "    \n",
    "    # 지표 계산\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred_valid))\n",
    "    mae = mean_absolute_error(y_true, y_pred_valid)\n",
    "    r2 = r2_score(y_true, y_pred_valid)\n",
    "    \n",
    "    metrics = {\n",
    "        \"RMSE\": float(rmse), \n",
    "        \"MAE\": float(mae), \n",
    "        \"R2\": float(r2),\n",
    "        \"CV_RMSE\": float(mean_cv_rmse)\n",
    "    }\n",
    "    \n",
    "    with open(output_dir/f\"metrics_{target}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"{target} 최종 평가 지표: {metrics}\")\n",
    "    \n",
    "    return final_model, metrics, X_valid, y_true, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───── 4. 모델 해석 및 시각화 함수 ─────\n",
    "def analyze_model(model, X, y_true, y_pred, features, output_dir, target):\n",
    "    \"\"\"모델 해석 및 시각화\"\"\"\n",
    "    logger.info(f\"{target} 모델 해석 시작\")\n",
    "    \n",
    "    # SHAP 분석\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    \n",
    "    # SHAP 요약 플롯\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    shap.summary_plot(shap_values, X, show=False, max_display=20)\n",
    "    plt.title(f'SHAP Summary for {target.upper()}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir/f\"shap_summary_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # SHAP 의존성 플롯 (상위 5개 특성)\n",
    "    feature_importance = np.abs(shap_values).mean(0)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_features = feature_importance_df['feature'].head(5).tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        shap.dependence_plot(feature, shap_values, X, show=False)\n",
    "        plt.title(f'SHAP Dependence Plot: {feature} for {target.upper()}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir/f\"shap_dependence_{feature}_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 특성 중요도 시각화\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    lgb.plot_importance(model, max_num_features=20, figsize=(14, 10))\n",
    "    plt.title(f'{target.upper()} LightGBM Feature Importance', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir/f\"feature_importance_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 예측 vs 실제 산점도\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel(f'Actual {target.upper()}', fontsize=12)\n",
    "    plt.ylabel(f'Predicted {target.upper()}', fontsize=12)\n",
    "    plt.title(f'Actual vs Predicted {target.upper()} (R² = {r2_score(y_true, y_pred):.4f})', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir/f\"actual_vs_predicted_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 잔차 분석\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(f'Predicted {target.upper()}', fontsize=12)\n",
    "    plt.ylabel('Residuals', fontsize=12)\n",
    "    plt.title(f'Residual Plot for {target.upper()}', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir/f\"residual_plot_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 잔차 분포\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.xlabel('Residuals', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(f'Residual Distribution for {target.upper()}', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir/f\"residual_distribution_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"{target} 모델 해석 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_seasonal_performance(df, model, features, output_dir, target):\n",
    "    \"\"\"계절별 모델 성능 분석\"\"\"\n",
    "    logger.info(f\"{target} 계절별 성능 분석 시작\")\n",
    "    \n",
    "    # 계절 정의\n",
    "    seasons = {\n",
    "        'spring': df[(df['date'].dt.month >= 3) & (df['date'].dt.month <= 5)],\n",
    "        'summer': df[(df['date'].dt.month >= 6) & (df['date'].dt.month <= 8)],\n",
    "        'fall': df[(df['date'].dt.month >= 9) & (df['date'].dt.month <= 11)],\n",
    "        'winter': df[(df['date'].dt.month == 12) | (df['date'].dt.month <= 2)]\n",
    "    }\n",
    "    \n",
    "    season_metrics = {}\n",
    "    \n",
    "    for season_name, season_df in seasons.items():\n",
    "        if len(season_df) > 0:\n",
    "            logger.info(f\"{season_name} 분석 시작: {len(season_df)} 행\")\n",
    "            \n",
    "            X_season = season_df[features]\n",
    "            y_season = season_df[target]\n",
    "            \n",
    "            # 유효한 행만 선택\n",
    "            valid_idx = y_season.notna()\n",
    "            X_valid = X_season[valid_idx]\n",
    "            y_valid = y_season[valid_idx]\n",
    "            \n",
    "            if len(y_valid) > 0:\n",
    "                # 모델 예측\n",
    "                y_pred = model.predict(X_valid)\n",
    "                \n",
    "                # 메트릭 계산\n",
    "                rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "                mae = mean_absolute_error(y_valid, y_pred)\n",
    "                r2 = r2_score(y_valid, y_pred)\n",
    "                \n",
    "                season_metrics[season_name] = {\"RMSE\": float(rmse), \"MAE\": float(mae), \"R2\": float(r2)}\n",
    "                \n",
    "                with open(output_dir/f\"{season_name}_{target}_metrics.json\", \"w\") as f:\n",
    "                    json.dump(season_metrics[season_name], f, indent=2)\n",
    "                \n",
    "                logger.info(f\"{season_name} 성능: RMSE={rmse:.4f}, R2={r2:.4f}\")\n",
    "                \n",
    "                # 계절별 SHAP 값\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                season_shap_values = explainer.shap_values(X_valid)\n",
    "                \n",
    "                plt.figure(figsize=(14, 10))\n",
    "                shap.summary_plot(season_shap_values, X_valid, show=False, max_display=15)\n",
    "                plt.title(f'SHAP Summary for {season_name.capitalize()} - {target.upper()}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(output_dir/f\"{season_name}_{target}_shap_summary.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "    \n",
    "    # 계절별 성능 비교 시각화\n",
    "    if season_metrics:\n",
    "        seasons_order = ['spring', 'summer', 'fall', 'winter']\n",
    "        available_seasons = [s for s in seasons_order if s in season_metrics]\n",
    "        \n",
    "        metrics_to_plot = ['RMSE', 'R2']\n",
    "        \n",
    "        for metric in metrics_to_plot:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            values = [season_metrics[s][metric] for s in available_seasons]\n",
    "            \n",
    "            if metric == 'RMSE':\n",
    "                title = f'계절별 RMSE 비교 ({target.upper()})'\n",
    "                ylabel = 'RMSE (낮을수록 좋음)'\n",
    "                color = 'skyblue'\n",
    "            else:\n",
    "                title = f'계절별 R² 비교 ({target.upper()})'\n",
    "                ylabel = 'R² Score (높을수록 좋음)'\n",
    "                color = 'lightgreen'\n",
    "            \n",
    "            bars = plt.bar(available_seasons, values, color=color, alpha=0.7)\n",
    "            \n",
    "            # 값 표시\n",
    "            for bar, val in zip(bars, values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, val + 0.01, \n",
    "                        f'{val:.3f}', ha='center', fontsize=11)\n",
    "            \n",
    "            plt.title(title, fontsize=16)\n",
    "            plt.ylabel(ylabel, fontsize=12)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_dir/f\"seasonal_{metric.lower()}_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    logger.info(f\"{target} 계절별 성능 분석 완료\")\n",
    "\n",
    "def analyze_domestic_foreign_impact(df, features, target, output_dir, cv_splits=5):\n",
    "    \"\"\"국내 오염원과 국외 오염원의 영향 분석\"\"\"\n",
    "    logger.info(f\"{target} 국내외 오염원 영향 분석 시작\")\n",
    "    \n",
    "    # 특성 분류\n",
    "    domestic_cols = [col for col in features \n",
    "                    if not (col.endswith('_pm10') or col.endswith('_pm25') or \n",
    "                           col.endswith('_pm10_lag1') or col.endswith('_pm25_lag1') or\n",
    "                           col.endswith('_pm10_lag2') or col.endswith('_pm25_lag2') or\n",
    "                           col.endswith('_pm10_lag3') or col.endswith('_pm25_lag3') or\n",
    "                           col.endswith('_pm10_ma3') or col.endswith('_pm25_ma3') or\n",
    "                           col.endswith('_pm10_ma7') or col.endswith('_pm25_ma7'))]\n",
    "    \n",
    "    foreign_cols = [col for col in features \n",
    "                   if (col.endswith('_pm10') or col.endswith('_pm25') or \n",
    "                       col.endswith('_pm10_lag1') or col.endswith('_pm25_lag1') or\n",
    "                       col.endswith('_pm10_lag2') or col.endswith('_pm25_lag2') or\n",
    "                       col.endswith('_pm10_lag3') or col.endswith('_pm25_lag3') or\n",
    "                       col.endswith('_pm10_ma3') or col.endswith('_pm25_ma3') or\n",
    "                       col.endswith('_pm10_ma7') or col.endswith('_pm25_ma7'))]\n",
    "    \n",
    "    logger.info(f\"국내 특성 수: {len(domestic_cols)}, 국외 특성 수: {len(foreign_cols)}\")\n",
    "    \n",
    "    # 시계열 교차 검증 설정\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "    \n",
    "    # 모델 설정 (간소화된 파라미터)\n",
    "    base_params = {\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 63, \n",
    "        'learning_rate': 0.05, \n",
    "        'n_estimators': 500, \n",
    "        'subsample': 0.8, \n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # 결과 저장용 딕셔너리\n",
    "    results = {}\n",
    "    \n",
    "    # 1. 전체 특성 모델\n",
    "    if len(features) > 0:\n",
    "        X_all = df[features]\n",
    "        y = df[target]\n",
    "        \n",
    "        # 유효한 행만 선택\n",
    "        valid_idx = y.notna()\n",
    "        X_all_valid = X_all[valid_idx]\n",
    "        y_valid = y[valid_idx]\n",
    "        \n",
    "        model_all = lgb.LGBMRegressor(**base_params)\n",
    "        \n",
    "        cv_scores_all = []\n",
    "        for tr, va in tscv.split(X_all_valid):\n",
    "            model_all.fit(\n",
    "                X_all_valid.iloc[tr], y_valid.iloc[tr],\n",
    "                eval_set=[(X_all_valid.iloc[va], y_valid.iloc[va])],\n",
    "                eval_metric=\"rmse\",\n",
    "                callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model_all.predict(X_all_valid.iloc[va])\n",
    "            rmse = np.sqrt(mean_squared_error(y_valid.iloc[va], y_pred))\n",
    "            cv_scores_all.append(rmse)\n",
    "        \n",
    "        # 전체 데이터로 최종 모델 학습\n",
    "        model_all.fit(X_all_valid, y_valid)\n",
    "        y_pred_all = model_all.predict(X_all_valid)\n",
    "        \n",
    "        rmse_all = np.sqrt(mean_squared_error(y_valid, y_pred_all))\n",
    "        r2_all = r2_score(y_valid, y_pred_all)\n",
    "        \n",
    "        results['전체 특성'] = {\n",
    "            'RMSE': float(rmse_all),\n",
    "            'R2': float(r2_all),\n",
    "            'CV_RMSE': float(np.mean(cv_scores_all))\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"전체 특성 모델 성능: RMSE={rmse_all:.4f}, R2={r2_all:.4f}\")\n",
    "    \n",
    "    # 2. 국내 오염원만 사용한 모델\n",
    "    if len(domestic_cols) > 0:\n",
    "        X_domestic = df[domestic_cols]\n",
    "        \n",
    "        # 유효한 행만 선택\n",
    "        X_domestic_valid = X_domestic[valid_idx]\n",
    "        \n",
    "        model_domestic = lgb.LGBMRegressor(**base_params)\n",
    "        \n",
    "        cv_scores_domestic = []\n",
    "        for tr, va in tscv.split(X_domestic_valid):\n",
    "            model_domestic.fit(\n",
    "                X_domestic_valid.iloc[tr], y_valid.iloc[tr],\n",
    "                eval_set=[(X_domestic_valid.iloc[va], y_valid.iloc[va])],\n",
    "                eval_metric=\"rmse\",\n",
    "                callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model_domestic.predict(X_domestic_valid.iloc[va])\n",
    "            rmse = np.sqrt(mean_squared_error(y_valid.iloc[va], y_pred))\n",
    "            cv_scores_domestic.append(rmse)\n",
    "        \n",
    "        # 전체 데이터로 최종 모델 학습\n",
    "        model_domestic.fit(X_domestic_valid, y_valid)\n",
    "        y_pred_domestic = model_domestic.predict(X_domestic_valid)\n",
    "        \n",
    "        rmse_domestic = np.sqrt(mean_squared_error(y_valid, y_pred_domestic))\n",
    "        r2_domestic = r2_score(y_valid, y_pred_domestic)\n",
    "        \n",
    "        results['국내 오염원만'] = {\n",
    "            'RMSE': float(rmse_domestic),\n",
    "            'R2': float(r2_domestic),\n",
    "            'CV_RMSE': float(np.mean(cv_scores_domestic))\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"국내 오염원만 모델 성능: RMSE={rmse_domestic:.4f}, R2={r2_domestic:.4f}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        with open(output_dir/f\"domestic_only_{target}_metrics.json\", \"w\") as f:\n",
    "            json.dump(results['국내 오염원만'], f, indent=2)\n",
    "    \n",
    "    # 3. 국외 오염원만 사용한 모델\n",
    "    if len(foreign_cols) > 0:\n",
    "        X_foreign = df[foreign_cols]\n",
    "        \n",
    "        # 유효한 행만 선택\n",
    "        X_foreign_valid = X_foreign[valid_idx]\n",
    "        \n",
    "        model_foreign = lgb.LGBMRegressor(**base_params)\n",
    "        \n",
    "        cv_scores_foreign = []\n",
    "        for tr, va in tscv.split(X_foreign_valid):\n",
    "            model_foreign.fit(\n",
    "                X_foreign_valid.iloc[tr], y_valid.iloc[tr],\n",
    "                eval_set=[(X_foreign_valid.iloc[va], y_valid.iloc[va])],\n",
    "                eval_metric=\"rmse\",\n",
    "                callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model_foreign.predict(X_foreign_valid.iloc[va])\n",
    "            rmse = np.sqrt(mean_squared_error(y_valid.iloc[va], y_pred))\n",
    "            cv_scores_foreign.append(rmse)\n",
    "        \n",
    "        # 전체 데이터로 최종 모델 학습\n",
    "        model_foreign.fit(X_foreign_valid, y_valid)\n",
    "        y_pred_foreign = model_foreign.predict(X_foreign_valid)\n",
    "        \n",
    "        rmse_foreign = np.sqrt(mean_squared_error(y_valid, y_pred_foreign))\n",
    "        r2_foreign = r2_score(y_valid, y_pred_foreign)\n",
    "        \n",
    "        results['국외 오염원만'] = {\n",
    "            'RMSE': float(rmse_foreign),\n",
    "            'R2': float(r2_foreign),\n",
    "            'CV_RMSE': float(np.mean(cv_scores_foreign))\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"국외 오염원만 모델 성능: RMSE={rmse_foreign:.4f}, R2={r2_foreign:.4f}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        with open(output_dir/f\"foreign_only_{target}_metrics.json\", \"w\") as f:\n",
    "            json.dump(results['국외 오염원만'], f, indent=2)\n",
    "    \n",
    "    # 국내외 오염원 영향 비교 시각화\n",
    "    if len(results) > 1:\n",
    "        # R2 비교\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        r2_values = [results[k]['R2'] for k in results.keys()]\n",
    "        \n",
    "        bars = plt.bar(list(results.keys()), r2_values, color=['#3498db', '#2ecc71', '#e74c3c'][:len(results)])\n",
    "        \n",
    "        # 값 표시\n",
    "        for bar, val in zip(bars, r2_values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, val + 0.01, \n",
    "                    f'{val:.3f}', ha='center', fontsize=11)\n",
    "        \n",
    "        plt.title(f'{target.upper()} 국내외 오염원 영향 비교 (R²)', fontsize=16)\n",
    "        plt.ylabel('R² Score (높을수록 좋음)', fontsize=12)\n",
    "        plt.ylim(0, max(r2_values) * 1.2)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir/f\"domestic_foreign_comparison_r2_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # RMSE 비교\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        rmse_values = [results[k]['RMSE'] for k in results.keys()]\n",
    "        \n",
    "        bars = plt.bar(list(results.keys()), rmse_values, color=['#3498db', '#2ecc71', '#e74c3c'][:len(results)])\n",
    "        \n",
    "        # 값 표시\n",
    "        for bar, val in zip(bars, rmse_values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, val + 0.5, \n",
    "                    f'{val:.2f}', ha='center', fontsize=11)\n",
    "        \n",
    "        plt.title(f'{target.upper()} 국내외 오염원 영향 비교 (RMSE)', fontsize=16)\n",
    "        plt.ylabel('RMSE (낮을수록 좋음)', fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir/f\"domestic_foreign_comparison_rmse_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"{target} 국내외 오염원 영향 분석 완료\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_analysis(target, output_dir):\n",
    "    \"\"\"회귀분석 실행 함수\"\"\"\n",
    "    logger.info(f\"\\n===== {target.upper()} 회귀분석 시작 =====\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    pm_kr_raw, pm_cn_raw, met_kr, master = load_data()\n",
    "    \n",
    "    # 2. 데이터 전처리\n",
    "    pm_kr = tidy_pm(pm_kr_raw)\n",
    "    pm_cn = tidy_pm(pm_cn_raw)\n",
    "    \n",
    "    # 3. 시계열 보간\n",
    "    pm_kr = interpolate_city(pm_kr)\n",
    "    pm_cn = interpolate_city(pm_cn)\n",
    "    \n",
    "    # 4. 중국 시차 특성 생성\n",
    "    pm_cn_features = prepare_china_features(pm_cn)\n",
    "    \n",
    "    # 5. 마스터 데이터 처리\n",
    "    master_daily_pm10, master_daily_pm25 = process_master_data(master)\n",
    "    \n",
    "    # 6. 데이터셋 병합\n",
    "    if target == \"pm10\":\n",
    "        df = merge_datasets(pm_kr, met_kr, master_daily_pm10, pm_cn_features, \"PM10\")\n",
    "    else:\n",
    "        df = merge_datasets(pm_kr, met_kr, master_daily_pm25, pm_cn_features, \"PM2.5\")\n",
    "    \n",
    "    # 7. 특성 엔지니어링\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # 8. 특성 선택\n",
    "    X, y, features = select_features(df, target)\n",
    "    \n",
    "    # 9. 시계열 교차 검증 설정\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # 10. 하이퍼파라미터 최적화\n",
    "    best_params = optimize_hyperparameters(X, y, cv)\n",
    "    \n",
    "    # 11. 모델 학습 및 평가\n",
    "    model, metrics, X_valid, y_true, y_pred = train_evaluate_model(\n",
    "        X, y, best_params, output_dir, target, cv_splits=5\n",
    "    )\n",
    "    \n",
    "    # 12. 모델 해석 및 시각화\n",
    "    analyze_model(model, X_valid, y_true, y_pred, features, output_dir, target)\n",
    "    \n",
    "    # 13. 계절별 성능 분석\n",
    "    analyze_seasonal_performance(df, model, features, output_dir, target)\n",
    "    \n",
    "    # 14. 국내외 오염원 영향 분석\n",
    "    impact_results = analyze_domestic_foreign_impact(df, features, target, output_dir)\n",
    "    \n",
    "    # 15. 데이터셋 저장\n",
    "    df.to_parquet(output_dir/f\"final_dataset_{target}.parquet\", index=False)\n",
    "    \n",
    "    logger.info(f\"===== {target.upper()} 회귀분석 완료 =====\\n\")\n",
    "    return model, metrics, impact_results\n",
    "\n",
    "logger.info(\"회귀분석 시작\")\n",
    "    \n",
    "# PM2.5 회귀분석\n",
    "model_pm25, metrics_pm25, impact_pm25 = run_regression_analysis(\"pm25\", OUT_PM25)\n",
    "    \n",
    "# PM10 회귀분석\n",
    "model_pm10, metrics_pm10, impact_pm10 = run_regression_analysis(\"pm10\", OUT_PM10)\n",
    "    \n",
    "# 결과 요약\n",
    "logger.info(\"\\n===== 회귀분석 결과 요약 =====\")\n",
    "logger.info(f\"PM2.5 회귀분석 결과: {metrics_pm25}\")\n",
    "logger.info(f\"PM10 회귀분석 결과: {metrics_pm10}\")\n",
    "logger.info(\"결과는 다음 폴더에 저장되었습니다:\")\n",
    "logger.info(f\"PM2.5 결과: {OUT_PM25}\")\n",
    "logger.info(f\"PM10 결과: {OUT_PM10}\")\n",
    "logger.info(\"===== 분석 완료 =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
